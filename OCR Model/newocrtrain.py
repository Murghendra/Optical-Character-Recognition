# -*- coding: utf-8 -*-
"""newocrtrain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i1uXd4mck3XnVMJQCJF1P0ea0kUceEKB
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

"""#Data preprocessing
## training set preprocessing
"""

input_shape = (64, 64, 3)

# Function to preprocess images and resize them
def preprocess_image(image, label):
    image = tf.image.resize(image, (64, 64))  # Resize images to the desired size
    return image, label

training_set = tf.keras.utils.image_dataset_from_directory(
    '/content/drive/MyDrive/newocr dataset/training_data',
    labels = "inferred",
    label_mode = "categorical" ,
    class_names = None,
    color_mode = "rgb" ,
    batch_size = 32,
    image_size = (64,64),
    shuffle = True,
    seed = None,
    validation_split = None,
    subset = None,
    interpolation = "bilinear",
    follow_links = False,
    crop_to_aspect_ratio = False
)

"""##validation set preprocessing"""

validation_set = tf.keras.utils.image_dataset_from_directory(
    '/content/drive/MyDrive/newocr dataset/validation_data',
    labels = "inferred",
    label_mode = "categorical" ,
    class_names = None,
    color_mode = "rgb" ,
    batch_size = 32,
    image_size = (64,64),
    shuffle = True,
    seed = None,
    validation_split = None,
    subset = None,
    interpolation = "bilinear",
    follow_links = False,
    crop_to_aspect_ratio = False
)

"""#Building model"""

cnn = tf.keras.models.Sequential()

cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation='relu' , input_shape =[64 ,64,3]))
cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation='relu' ))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))

cnn.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, activation='relu' ))
cnn.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, activation='relu' ))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))

cnn.add(tf.keras.layers.Flatten())

cnn.add(tf.keras.layers.Dense(units=512,activation='relu'))

cnn.add(tf.keras.layers.Dense(units=256,activation='relu'))

cnn.add(tf.keras.layers.Dropout(0.5))

cnn.add(tf.keras.layers.Dense(units=36,activation='softmax'))

"""#compiling and training phase"""

cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

cnn.summary()

training_history = cnn.fit(x = training_set,validation_data = validation_set, epochs=20 )

cnn.save('trained_model.h5')

training_history.history

#recording history
import json
with open('training_hist.json','w') as f:
  json.dump(training_history.history,f)

print(training_history.history.keys())

print("validation set accuaracy: {} % ".format(training_history.history['val_accuracy'][-1]*100))

training_history.history['accuracy']

epochs = [i for i in range(1,21)]
plt.plot(epochs,training_history.history['accuracy'],color='red')
plt.xlabel('No of Epochs')
plt.ylabel('Training Accuracy')
plt.title('Visualization of training accuracy result ')
plt.show()

plt.plot(epochs,training_history.history['val_accuracy'],color='blue')
plt.xlabel('No of Epochs')
plt.ylabel('Validation Accuracy')
plt.title('Visualization of validation accuracy result ')
plt.show()

training_loss,training_accuracy = cnn.evaluate(training_set)

val_loss,val_accuracy = cnn.evaluate(validation_set)

test_set = tf.keras.utils.image_dataset_from_directory(
    '/content/drive/MyDrive/newocr dataset/testing_data',
    labels = "inferred",
    label_mode = "categorical" ,
    class_names = None,
    color_mode = "rgb" ,
    batch_size = 32,
    image_size = (64,64),
    shuffle = True,
    seed = None,
    validation_split = None,
    subset = None,
    interpolation = "bilinear",
    follow_links = False,
    crop_to_aspect_ratio = False
)

test_loss,test_accuracy = cnn.evaluate(test_set)

